$ oc explain limitranges.spec.limits
$ oc explain deployment.spec.template.spec.nodeSelector
$ oc api-resources

Recordar: meter recursos para un container
$oc set resource 


oc cluster-info

oc debug node/nombre-nodo
oc adm top nodes  ---- Presenta el uso actual de la CPU y la memoria
oc describe node my-node-name
oc get clusterversion
oc describe clusterversion
oc get clusteroperators
oc adm node-logs -u kubelet my-node-name
oc adm node-logs my-node-name

Apertura de un prompt de shell en un nodo de OpenShift
[user@demo ~]$ oc debug node/my-node-name
...output omitted...
sh-4.2# chroot /host
sh-4.2# systemctl is-active kubelet
active

Ver contenedores dentro de un nodo
oc describe node

[user@demo ~]$ oc debug node/my-node-name
...output omitted...
sh-4.2# chroot /host
sh-4.2# crictl ps

Logs de un contenedor
oc logs my-pod-name
oc logs my-pod-name -c my-container-name    ---- Si un pod tiene mas de un contenedor

Creación de pods de solución de problemas
oc debug deployment/my-deployment-name --as-root

Crear un pod debug con imagen modificada para tener herramientas
$ oc debug -t deployment/todo-http --image registry.access.redhat.com/ubi8/ubi:8.0

Ver si existe una imagen
skopeo inspect docker://registry.access.redhat.com/rhscl/postgresql-96-rhel7:1

Abrir una shell dentro de un pod
oc rsh nombre-de-mi-pod

Copia archivos locales en una ubicación dentro de un pod
oc cp /local/path nombre-de-mi-pod:/container/path

Crea un túnel TCP entre local-port en su estación de trabajo y local-port en el pod.
oc port-forward nombre-de-mi-pod local-port:remote-port


Loglevel (de 6 hasta 10)
oc get pod --loglevel 6
oc get pod --loglevel 10

Descrubir token que usa mi oc
oc whoami -t
Se puede usar para imprimir la api mediante curl
curl 'https://api.ocp-lnjmqodepmrdnqq200716.do280.rht-eu.nextcle.com:6443/api/v1/' \
  -H "Authorization: Bearer $TOKEN" \
  --insecure
  
  
  
Actualización del recurso personalizado de OAuth
[user@demo ~]$ oc get -o yaml oauth cluster > oauth.yaml
...
spec:
 identityProviders:
 - name: my_htpasswd_provider
 mappingMethod: claim
 type: HTPasswd
 htpasswd:
 fileData:
 name: htpasswd-secret 
[user@demo ~]$ oc replace -f oauth.yaml



Creación de un secreto
Crear htpasswd
$ htpasswd -c -B -b /tmp/htpasswd student redhat123
Eliminar un usuario
$ htpasswd -D /tmp/htpasswd student

$ oc create secret generic htpasswd-secret \
 --from-file htpasswd=/tmp/htpasswd -n openshift-config


Actualizacion de secreto
[user@demo ~]$ oc create secret generic htpasswd-secret --from-file htpasswd=/tmp/htpasswd --dry-run -o yaml \
 | oc replace -n openshift-config -f -
 
$ watch oc get pods -n openshift-authentication

Extraer un htpasswd
$ oc extract secret/htpasswd-secret -n openshift-config \
--to - > /tmp/htpasswd

Eliminar usuario de identidad
$ htpasswd -D /tmp/htpasswd manager

$ oc create secret generic htpasswd-secret \
> --from-file htpasswd=/tmp/htpasswd --dry-run -o yaml \
> | oc replace -n openshift-config -f -

$ oc delete user manager

Buscar el usuario en la identidad ey eliminarlo
$ oc get identities | grep manager
my_htpasswd_provider:manager my_htpasswd_provider manager manager ...
$ oc delete identity my_htpasswd_provider:manager
identity.user.openshift.io "my_htpasswd_provider:manager" deleted

Asignación de privilegios administrativos
$ oc adm policy add-cluster-role-to-user cluster-admin student

Eliminar capacidad de crear nuevos proyectos a los usuarios de oauth
$ oc adm policy remove-cluster-role-from-group self-provisioner system:authenticated:oauth
Dar permiso de crear nuevos proyectos al grupo leaders
$ oc adm policy add-cluster-role-to-group self-provisioner leaders


Control de acceso


admin ---- Los usuarios con este rol pueden administrar todos los recursos del proyecto, incluso otorgar acceso al proyecto a otros usuarios.
basic-user ---- Los usuarios con este rol tienen acceso de lectura al proyecto.
cluster-admin ---- Los usuarios con este rol tienen acceso de superusuario a los recursos del clúster. Estos usuarios pueden realizar cualquier acción en el clúster y tienen control total de todos los proyectos.
cluster-status ---- Los usuarios con este rol pueden obtener información sobre el estado del clúster.
edit ---- Los usuarios con este rol pueden crear, modificar o eliminar recursos de aplicaciones comunes del proyecto, como opciones de configuración de servicios e implementación. No pueden actuar sobre recursos de administración, como cuotas y rangos límite, y no pueden administrar permisos de acceso al
proyecto.
self-provisioner ---- Los usuarios con este rol pueden crear nuevos proyectos. Este es un rol de clúster, no un rol de proyecto.
view ---- Los usuarios con este rol pueden ver los recursos del proyecto, pero no pueden modificar los recursos del proyecto.

$ oc get clusterrolebinding
$ oc get rolebinding

$ oc adm policy add-role-to-user basic-user dev -n wordpress

Utilizar secretos para env
$ oc create secret generic mysql \
> --from-literal user=myuser --from-literal password=redhat123 \
> --from-literal database=test_secrets --from-literal hostname=mysql 

$ oc new-app --name mysql \
> --docker-image registry.access.redhat.com/rhscl/mysql-57-rhel7:5.7-47

$ oc set env dc/mysql --prefix MYSQL_ \
> --from secret/mysql


Restricciones de contexto de seguridad (SCC)

oc get scc
oc get serviceaccount
$ oc create serviceaccount gitlab-sa
$ oc adm policy add-scc-to-user anyuid -z gitlab-sa
$ oc set serviceaccount deploymentconfig gitlab gitlab-sa

Ver todas las restricciones de contexto de seguridad que se pueden usar para sortear las limitaciones de un contenedor
$ oc get pod podname -o yaml | oc adm policy scc-subject-review -f -


Network

IMPORTANTE: Verificar que los svc tienen bien los selectores para el correcto direccionamiento (label name igual que el nombre svc)
			Crear pod debug (oc debug -t deployment {nombre}) y probar si llega tanto a la ip de cluster como a la privada.
			Ip privada se saca con oc get nodes -o wide
			Ip cluster se saca con oc get service
			

Hay 3 modos de red: subnet, multitenant y NetworkPolicy (default)
$ oc describe dns.operator/default

Gestion de trafico: Ingress (resource), External load balancer (service type), Service external IP (service type), NodePort (service type)

Rutas seguras de OpenShift:
	Edge: El enrutador otorga los certificados TLS, por lo que debe configurarlos en la ruta; de lo contrario, OpenShift asigna su propio certificado al enrutador para la finalización de TLS. Cifran el tráfico entre el cliente y el enrutador
	Pass-through: el tráfico cifrado se envía directamente al pod de destino,  la aplicación es responsable de proporcionar certificados para el tráfico
	Re-encryption:  el enrutador finaliza el TLS con un certificado y, luego, recifra su conexión al extremo
	

$ oc create route edge https --service todo-http --hostname https.${RHT_OCP4_WILDCARD_DOMAIN}
$ oc extract secrets/router-ca --keys tls.crt -n openshift-ingress-operator

AYUDA: $ man req
$ openssl genrsa -out training.key 2048
$ openssl req -new -subj "/C=US/ST=North Carolina/L=Raleigh/O=Red Hat/\CN=https.${RHT_OCP4_WILDCARD_DOMAIN}" -key training.key -out training.csr
$ openssl x509 -req -in training.csr -passin file:passphrase.txt -CA training-CA.pem -CAkey training-CA.key -CAcreateserial -out training.crt -days 1825 -sha256 -extfile training.ext

$ oc create route edge --service api-frontend --hostname https.${RHT_OCP4_WILDCARD_DOMAIN} --key api.key --cert api.crt
$ oc create secret tls todo-certs --cert=certs/training.crt --key=certs/training.key
$ oc create route passthrough https --service todo-https --port 8443 --hostname https.${RHT_OCP4_WILDCARD_DOMAIN}
$ oc set volume dc/lamp --add --name=tls-certs --type=secret --secret-name=todo-certs --mount-path=/usr/local/etc/ssl/certs

Con passthrough los certificados se pasan al pod mediante un volume
spec:
...
volumes:
- name: tls-certs
  secret:
    secretName: todo-certs
...


Control de programacion


1. Filtrado de nodos. 
	Taint en nodos
	tolerancia en pods
	
2. Priorización de la lista de nodos filtrada.
	Un uso común para las reglas de afinidad es programar pods relacionados para que estén
	cerca uno del otro por motivos de rendimiento. Un ejemplo es usar el mismo backbone de red
	para pods que se sincronizan entre sí.
	Un uso común para las reglas de antiafinidad es programar pods relacionados para
	que no estén tan cerca uno del otro, para una alta disponibilidad. Un ejemplo es evitar la
	programación de todos los pods de la misma aplicación al mismo nodo.

3. Selección del nodo más apto.
	La lista de candidatos se ordena según estas puntuaciones, y el nodo con la mayor puntuación
	se selecciona para alojar el pod. Si varios nodos tienen la misma puntuación alta, se
	selecciona uno con el método round robin.


Etiquetado de nodos
Agregar etiqueta
$ oc label node node1.us-west-1.compute.internal env=dev [--overwrite]
Eliminar etiqueta
$ oc label node node1.us-west-1.compute.internal env-
Mostrar el valor de una etiqueta en todos los nodos
$ oc get node -L failure-domain.beta.kubernetes.io/region -L env

Etiquetado de machines
Relacion de maquinas y nodos
$ oc get machines -n openshift-machine-api -o wide
Mostrar la machineset desde donde se generan las demás
$ oc get machineset -n openshift-machine-api
Modificar los tag para que cuando genere una nueva machine desde la machineset se genere ese tag
$ oc edit machineset ocp-qz7hf-worker-us-west-1b -n openshift-machine-api
...
spec:
  metadata:
    creationTimestamp: null
    labels:
      env: dev
   providerSpec:
...


$ oc adm new-project demo --node-selector "tier=1"
Agregar un selector de nodo predeterminado a un proyecto
$ oc annotate namespace demo openshift.io/node-selector="tier=2" --overwrite

Modificar deployment para que seletor env
$ oc label node ip-10-0-152-96.us-east-2.compute.internal env=prod
$ oc edit deployment/hello
...output omitted...
     terminationMessagePath: /dev/termination-log
     terminationMessagePolicy: File
   dnsPolicy: ClusterFirst
   nodeSelector:
     env: dev
   restartPolicy: Always
...output omitted...

IMPORTANTE: Ver si el deploy tiene nodeSelector y ver is los nodos tambien tienen los label
AYUDA: $ oc explain deployment.spec.template.spec.nodeSelector

Limite de recursos

LINK: https://access.redhat.com/documentation/en-us/openshift_container_platform/4.2/html/nodes/working-with-clusters#nodes-cluster-resource-configure-request-limit_nodes-cluster-resource-configure

Hay dos definiciones:
	- Definición de solicitudes
	- límites de recursos para pods

IMPORTANTE
$ oc set resources deployment hello-world-nginx --requests cpu=10m,memory=20Mi --limits cpu=80m,memory=100Mi

Aplicación de cuotas

Hay dos tipos de recursos:
	- Recuentos de objetos: La cantidad de recursos de Kubernetes, como pods, servicios y rutas.
	- Recursos de cómputo: La cantidad de recursos de hardware físicos y virtuales, como CPU, memoria y capacidad de almacenamiento.


Quota para proyecto

apiVersion: v1
kind: ResourceQuota
metadata:
 name: dev-quota
spec:
 hard:
   services: "10"
   cpu: "1300m"
   memory: "1.5Gi"
   requests.cpu: "50m"
   limit.cpu: "500m"

$ oc create --save-config -f dev-quota.yml

$ oc create quota dev-quota --hard services=10,cpu=1300,memory=1.5Gi
$ oc delete resourcequota QUOTA

IMPORTANTE: Revisar si un proyecto tiene quota
NOTA: Si hay 20 pods en running y se configura un quota de 15 pods, no se eliminan los 5 pods restantes. Solo hace efecto para los nuevos despliegues.

Quota de rango para pods

...
spec:
 limits:
 - type: "Pod"
   max:
     cpu: "500m"
     memory: "750Mi"
   min:
     cpu: "10m"
     memory: "5Mi"
 - type: "Container"
   default:
     cpu: "100m"
     memory: "100Mi"
   max:
     cpu: "500m"
     memory: "750Mi"
   min:
     cpu: "10m"
     memory: "5Mi"
...
$ oc create --save-config -f dev-limits.yml
$ oc describe limitrange dev-limits
$ oc delete limitrange dev-limits

AYUDA: $ oc explain limitranges.spec.limits



Cuotas a varios proyectos

$ oc create clusterquota user-qa \
 --project-annotation-selector openshift.io/requester=qa \
 --hard pods=12,secrets=20

$ oc create clusterquota env-qa \
 --project-label-selector environment=qa \
 --hard pods=10,services=5

$ oc delete clusterquota QUOTA


Escalado de aplicaciones

Escalamiento manual
$ oc scale --replicas 5 deployment/scale

Escalamiento automático (autoscaling) 
$ oc autoscale dc/hello --min 1 --max 10 --cpu-percent 80
$ oc get hpa


Escalamiento de cluster
Existe MachineAutoscaler y ClusterAutoscaler

Escalamiento manual
$ oc scale --replicas=2 machineset MACHINE-SET -n openshift-machine-api


Escalamiento automatico : hace falta MachineAutoscaler y ClusterAutoscaler

- Si el recurso MachineAutoscaler debe escalarse, comprueba si existe un recurso
ClusterAutoscaler. Si no existe, no se produce ningún escalamiento.
- Si existe un recurso ClusterAutoscaler, el recurso MachineAutoscaler evalúa
si la adición de la nueva máquina infringe cualquiera de los límites definidos en
ClusterAutoscaler.

ClusterAutoScaler: limita los nodos, memoria y cpu del cluster
MachineAutoScaler: limita el min y max de machines


ClusterAutoScaler

apiVersion: "autoscaling.openshift.io/v1"
kind: "ClusterAutoscaler"
metadata:
 name: "default"
spec:
 podPriorityThreshold: -10
 resourceLimits:
  maxNodesTotal: 6
scaleDown:
 enabled: true
 delayAfterAdd: 3m
 unneededTime: 3m


MachineAutoScaler

apiVersion: "autoscaling.openshift.io/v1beta1"
kind: "MachineAutoscaler"
metadata:
name: "scale-automatic"
namespace: "openshift-machine-api"
spec:
minReplicas: 1
maxReplicas: 3
scaleTargetRef:
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
name: MACHINE-SET-NAME
